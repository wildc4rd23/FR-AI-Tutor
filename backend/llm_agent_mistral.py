# backend/llm_agent_mistral.py - Optimierte Version
import os
import requests
import logging
import json

logger = logging.getLogger(__name__)

def get_scenario_system_prompt(scenario):
    """
    Erweiterte szenario-spezifische System-Prompts fÃ¼r bessere GesprÃ¤chsqualitÃ¤t
    """
    base_prompt = """Tu es un professeur de franÃ§ais expÃ©rimentÃ© qui aide des Ã©tudiants de niveau B1/B2. 
    
    RÃˆGLES IMPORTANTES:
    - RÃ©ponds TOUJOURS en franÃ§ais
    - Garde tes rÃ©ponses concises mais informatives (2-4 phrases maximum)
    - NE rÃ©ponds PAS par des phrases trop courtes ou des mots uniques
    - Corrige gentiment les erreurs sans Ãªtre condescendant
    - Pose TOUJOURS une question ouverte pour relancer la conversation
    - Utilise un vocabulaire appropriÃ© au niveau B1/B2
    - Sois encourageant, patient et bienveillant
    - Agis comme un vÃ©ritable partenaire de discussion
    """
    
    scenario_prompts = {
        "restaurant": base_prompt + """
        
        CONTEXTE SPÃ‰CIFIQUE - RESTAURANT:
        - Tu joues le rÃ´le d'un serveur franÃ§ais dans un restaurant traditionnel
        - L'Ã©tudiant est un client qui veut commander
        - Guide-le Ã  travers l'expÃ©rience complÃ¨te : accueil, menu, commande, paiement
        - Introduis du vocabulaire culinaire franÃ§ais authentique
        - CrÃ©e des situations rÃ©alistes (plats du jour, recommandations, allergies)
        - Utilise des expressions typiques des serveurs franÃ§ais
        - Propose des spÃ©cialitÃ©s rÃ©gionales franÃ§aises
        """,
        
        "loisirs": base_prompt + """
        
        CONTEXTE SPÃ‰CIFIQUE - LOISIRS:
        - Explore ses passions et dÃ©couvre de nouveaux hobbies ensemble
        - Partage des anecdotes sur les loisirs populaires en France
        - Introduis des activitÃ©s culturelles franÃ§aises (pÃ©tanque, randonnÃ©e, festivals)
        - Discute des diffÃ©rences culturelles dans les loisirs
        - Encourage Ã  partager ses expÃ©riences personnelles
        - Propose des activitÃ©s qu'il pourrait essayer en France
        """,
        
        "travail": base_prompt + """
        
        CONTEXTE SPÃ‰CIFIQUE - TRAVAIL:
        - Joue le rÃ´le d'un collÃ¨gue franÃ§ais ou d'un recruteur bienveillant
        - Discute des diffÃ©rences dans la culture du travail franÃ§aise
        - Introduis le vocabulaire professionnel franÃ§ais
        - Aborde les sujets : entretiens, rÃ©unions, congÃ©s, relations collÃ¨gues
        - Explique les spÃ©cificitÃ©s du systÃ¨me franÃ§ais (35h, RTT, etc.)
        - Aide Ã  prÃ©parer des situations professionnelles rÃ©elles
        """,
        
        "voyage": base_prompt + """
        
        CONTEXTE SPÃ‰CIFIQUE - VOYAGE:
        - Tu es un guide touristique franÃ§ais passionnÃ©
        - Fais dÃ©couvrir les rÃ©gions franÃ§aises avec enthousiasme
        - Partage des conseils pratiques et des secrets locaux
        - Introduis la gÃ©ographie, l'histoire et les traditions franÃ§aises
        - Aide Ã  planifier un voyage rÃ©aliste en France
        - Raconte des anecdotes sur les destinations franÃ§aises
        """,
        
        "libre": base_prompt + """
        
        CONTEXTE SPÃ‰CIFIQUE - CONVERSATION LIBRE:
        - Adapte-toi au sujet choisi par l'Ã©tudiant
        - Enrichis la conversation avec des Ã©lÃ©ments culturels franÃ§ais
        - Introduis naturellement du vocabulaire avancÃ©
        - Encourage l'expression d'opinions personnelles
        - CrÃ©e des connexions avec l'actualitÃ© franÃ§aise
        """
    }
    
    return scenario_prompts.get(scenario, scenario_prompts["libre"])

def get_scenario_starter(scenario):
    """
    Szenario-spezifische Starter-Prompts fÃ¼r natÃ¼rlichere GesprÃ¤che
    """
    starters = {
        "restaurant": """Bonjour et bienvenue dans notre restaurant ! Je vois que vous regardez la carte. 
        Puis-je vous expliquer nos spÃ©cialitÃ©s du jour ? Nous avons un excellent coq au vin aujourd'hui. 
        Avez-vous dÃ©jÃ  goÃ»tÃ© la cuisine franÃ§aise traditionnelle ?""",
        
        "loisirs": """Salut ! J'adore dÃ©couvrir ce que les gens font pendant leur temps libre. 
        Moi, le weekend dernier, j'ai fait une randonnÃ©e dans les Alpes - c'Ã©tait magnifique ! 
        Et vous, qu'est-ce que vous aimez faire quand vous avez du temps libre ?""",
        
        "travail": """Bonjour ! Je suis ravi de vous rencontrer. J'ai vu votre profil et votre parcours m'intÃ©resse beaucoup. 
        Parlez-moi un peu de votre expÃ©rience professionnelle. 
        Qu'est-ce qui vous motive le plus dans votre travail ?""",
        
        "voyage": """Bonjour et bienvenue ! Je suis guide touristique depuis 10 ans et j'adore faire dÃ©couvrir la France. 
        Dites-moi, c'est votre premiÃ¨re fois en France ou vous connaissez dÃ©jÃ  quelques rÃ©gions ? 
        Qu'est-ce qui vous attire le plus : la gastronomie, l'histoire, ou les paysages ?""",
        
        "libre": """Bonjour ! Je suis ravi de discuter avec vous aujourd'hui. 
        J'aime les conversations spontanÃ©es - on apprend toujours quelque chose d'intÃ©ressant ! 
        De quoi avez-vous envie de parler ? Ou voulez-vous que je vous pose une question pour commencer ?"""
    }
    
    return starters.get(scenario, starters["libre"])

def query_llm_mistral(prompt, history=None, max_tokens=150, temperature=0.7, scenario="libre"):
    """
    Optimierte LLM-Abfrage mit szenario-spezifischen Prompts
    """
    api_key = os.environ.get("MISTRAL_API_KEY")
    if not api_key:
        raise Exception("MISTRAL_API_KEY Umgebungsvariable fehlt")
    
    url = "https://api.mistral.ai/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    # Szenario-spezifischen System-Prompt verwenden
    system_prompt = get_scenario_system_prompt(scenario)
    messages = [{"role": "system", "content": system_prompt}]
    
    # Historie hinzufÃ¼gen (ohne Duplikate)
    if history:
        # Filtere System-Prompts aus der Historie
        filtered_history = [msg for msg in history if msg.get("role") != "system"]
        messages.extend(filtered_history)
    
    # Aktuellen Prompt hinzufÃ¼gen
    messages.append({"role": "user", "content": prompt})

    payload = {
        "model": "mistral-small",
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_p": 0.9,
        "stop": [".", "!", "?", "\n\n"]
    }
    
    # Debugging: Nur erste und letzte Nachricht loggen
    debug_messages = {
        "system": messages[0]["content"][:100] + "...",
        "last_user": messages[-1]["content"],
        "history_length": len(messages) - 2,  # Ohne System und aktuellen Prompt
        "scenario": scenario
    }
    logger.info("ðŸ§  LLM Mistral Request Summary: %s", json.dumps(debug_messages, indent=2, ensure_ascii=False))

    try:
        response = requests.post(url, headers=headers, json=payload, timeout=30)
        response.raise_for_status()
        
        result = response.json()
        content = result["choices"][0]["message"]["content"].strip()
        
        # Nachbearbeitung fÃ¼r TTS
        content = post_process_response(content, max_chars=200)
        
        logger.info(f"âœ… Mistral response: {len(content)} characters")
        return content
        
    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ Mistral API error: {str(e)}")
        raise Exception(f"Mistral API Fehler: {str(e)}")
    except KeyError as e:
        logger.error(f"âŒ Unexpected response format: {result}")
        raise Exception("Unerwartetes Antwortformat von Mistral API")

def get_intro_for_scenario(scenario):
    """
    Direkte RÃ¼ckgabe des Starter-Texts ohne LLM-Aufruf
    """
    return get_scenario_starter(scenario)

def post_process_response(text, max_chars=200):
    """
    Verbesserte Nachbearbeitung fÃ¼r TTS-Optimierung
    """
    # Bereinigung
    text = " ".join(text.split())
    
    # LÃ¤ngenoptimierung
    if len(text) > max_chars:
        sentences = text.split('. ')
        result = ""
        
        for sentence in sentences:
            if len(result + sentence + '. ') <= max_chars:
                result += sentence + '. '
            else:
                break
        
        if not result.strip():
            result = text[:max_chars-3] + "..."
        
        text = result.strip()
    
    # Satzende sicherstellen
    if text and not text.endswith(('.', '!', '?')):
        text += '.'
    
    return text

def query_llm_for_scenario(prompt, scenario="libre", history=None, max_tokens=160):
    """
    Szenario-spezifische LLM-Abfrage mit optimierten Parametern
    """
    scenario_configs = {
        "restaurant": {"max_tokens": 120, "temperature": 0.6},
        "loisirs": {"max_tokens": 160, "temperature": 0.8},
        "travail": {"max_tokens": 140, "temperature": 0.5},
        "voyage": {"max_tokens": 150, "temperature": 0.7},
        "libre": {"max_tokens": 150, "temperature": 0.7}
    }
    
    config = scenario_configs.get(scenario, scenario_configs["libre"])
    return query_llm_mistral(prompt, history=history, scenario=scenario, **config)