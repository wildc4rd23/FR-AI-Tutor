# backend/llm_agent_mistral.py - Optimierte Version
import os
import requests
import logging
import json

logger = logging.getLogger(__name__)

# Modifizierte get_scenario_system_prompt, um optional eine Anweisung fÃ¼r die erste Antwort zu akzeptieren

def get_scenario_system_prompt(scenario, first_turn_instruction_example=None):

    """
    Erweiterte szenario-spezifische System-Prompts fÃ¼r bessere GesprÃ¤chsqualitÃ¤t.
    Kann optional eine Anweisung fÃ¼r die erste Antwort enthalten.
    """
    base_prompt = """Tu es un professeur de franÃ§ais expÃ©rimentÃ© qui aide des Ã©tudiants de niveau B1/B2. 
    
    RÃˆGLES IMPORTANTES:
    - RÃ©ponds TOUJOURS en franÃ§ais
    - Garde tes rÃ©ponses concises mais informatives (2-4 phrases maximum)
    - NE rÃ©ponds PAS par des phrases trop courtes ou des mots uniques
    - Corrige gentiment les erreurs sans Ãªtre condescendant
    - Pose TOUJOURS une question ouverte pour relancer la conversation
    - Utilise un vocabulaire appropriÃ© au niveau B1/B2
    - Sois encourageant, patient et bienveillant
    - Agis comme un vÃ©ritable partenaire de discussion
    """
    logger.info(f"Base System Prompt geladen (Anfang): {base_prompt[:100]}...") # Loggen des Base Prompts

    scenario_prompts = {
        "restaurant": base_prompt + """
        
        CONTEXTE SPÃ‰CIFIQUE - RESTAURANT:
        - Tu joues le rÃ´le d'un serveur franÃ§ais dans un restaurant traditionnel
        - L'Ã©tudiant est un client qui veut commander
        - Guide-le Ã  travers l'expÃ©rience complÃ¨te : accueil, menu, commande, paiement
        - Introduis du vocabulaire culinaire franÃ§ais authentique
        - CrÃ©e des situations rÃ©alistes (plats du jour, recommandations, allergies)
        - Utilise des expressions typiques des serveurs franÃ§ais
        - Propose des spÃ©cialitÃ©s rÃ©gionales franÃ§aises
        """,

        "faire_les_courses": base_prompt + """
        
        CONTEXTE SPÃ‰CIFIQUE - AU SUPERMARCHÃ‰:
        - Tu joues le rÃ´le d'un vendeur
        """,

         "visite_chez_le_mÃ©decin": base_prompt + """
        
        CONTEXTE SPÃ‰CIFIQUE - VISITE CHEZ LE MÃ‰DECIN        
        - Tu joues le rÃ´le d'un docteur
        """,
        
        "loisirs": base_prompt + """
        
        CONTEXTE SPÃ‰CIFIQUE - LOISIRS:
        - Explore ses passions et dÃ©couvre de nouveaux hobbies ensemble
        - Partage des anecdotes sur les loisirs populaires en France
        - Introduis des activitÃ©s culturelles franÃ§aises (pÃ©tanque, randonnÃ©e, festivals)
        - Discute des diffÃ©rences culturelles dans les loisirs
        - Encourage Ã  partager ses expÃ©riences personnelles
        - Propose des activitÃ©s qu'il pourrait essayer en France
        """,
        
        "travail": base_prompt + """
        
        CONTEXTE SPÃ‰CIFIQUE - TRAVAIL:
        - Joue le rÃ´le d'un collÃ¨gue franÃ§ais ou d'un recruteur bienveillant
        - Discute des diffÃ©rences dans la culture du travail franÃ§aise
        - Introduis le vocabulaire professionnel franÃ§ais
        - Aborde les sujets : entretiens, rÃ©unions, congÃ©s, relations collÃ¨gues
        - Explique les spÃ©cificitÃ©s du systÃ¨me franÃ§ais (35h, RTT, etc.)
        - Aide Ã  prÃ©parer des situations professionnelles rÃ©elles
        """,
        
        "voyage": base_prompt + """
        
        CONTEXTE SPÃ‰CIFIQUE - VOYAGE:
        - Tu es un guide touristique franÃ§ais passionnÃ©
        - Fais dÃ©couvrir les rÃ©gions franÃ§aises avec enthousiasme
        - Partage des conseils pratiques et des secrets locaux
        - Introduis la gÃ©ographie, l'histoire et les traditions franÃ§aises
        - Aide Ã  planifier un voyage rÃ©aliste en France
        - Raconte des anecdotes sur les destinations franÃ§aises
        """,
        
        "libre": base_prompt + """
        
        CONTEXTE SPÃ‰CIFIQUE - CONVERSATION LIBRE:
        - Adapte-toi au sujet choisi par l'Ã©tudiant
        - Enrichis la conversation avec des Ã©lÃ©ments culturels franÃ§ais
        - Introduis naturellement du vocabulaire avancÃ©
        - Encourage l'expression d'opinions personnelles
        - CrÃ©e des connexions avec l'actualitÃ© franÃ§aise
        """
    }
    
    selected_prompt = scenario_prompts.get(scenario, scenario_prompts["libre"])

    if first_turn_instruction_example:

        selected_prompt += f"\n\nVOTRE PREMIÃˆRE RÃ‰PONSE DOIT RESSEMBLER Ã€ UN EXEMPLE COMME : Â« {first_turn_instruction_example} Â». Votre rÃ©ponse doit sembler naturelle et comme un dÃ©but direct de conversation."
        logger.info(f"Szenario-spezifischer Prompt mit Erst-Antwort-Anweisung (Anfang): {selected_prompt[:150]}...")

    else:

        logger.info(f"Szenario-spezifischer Prompt '{scenario}' ausgewÃ¤hlt (Anfang): {selected_prompt[:100]}...") # Loggen des Szenario Prompts

    return selected_prompt

def get_scenario_starter(scenario):
    """
    Definiert szenario-spezifische Starter-Prompts, die nun als Beispiele/Anweisungen dienen.
    """
    starters = {
        "restaurant": """Bonjour et bienvenue dans notre restaurant ! Je vois que vous regardez la carte. 
        Puis-je vous expliquer nos spÃ©cialitÃ©s du jour ? Nous avons un excellent coq au vin aujourd'hui. 
        Avez-vous dÃ©jÃ  goÃ»tÃ© la cuisine franÃ§aise traditionnelle ?""",

        "faire_les_courses": """Bonjour, comment puis-je vous aider? Vous cherchez quelque chose de spÃ©cifique?
        Nous vous proposons une offre spÃ©ciale aujourd'hui.""",

         "visite_chez_le_mÃ©decin": """Bonjour! Comment va votre jambe aujourd'hui? Avez-vous mal au genou? 
         Avez-vous besoin d'une ordonnance pour des mÃ©dicaments?""",
        
        "loisirs": """Salut ! J'adore dÃ©couvrir ce que les gens font pendant leur temps libre. 
        Moi, le weekend dernier, j'ai fait une randonnÃ©e dans les Alpes - c'Ã©tait magnifique ! 
        Et vous, qu'est-ce que vous aimez faire quand vous avez du temps libre ?""",
        
        "travail": """Bonjour ! Je suis ravi de vous rencontrer. Parlons un peu de votre parcours professionnel. 
        Quel est votre domaine d'expertise ? Qu'est-ce qui vous motive le plus dans votre travail ?""",

        "voyage": """Bienvenue ! Si vous pouviez voyager n'importe oÃ¹ en France, quelle rÃ©gion choisiriez-vous et pourquoi ?""",

        "libre": """Bonjour ! Je suis lÃ  pour pratiquer le franÃ§ais avec vous. Quel sujet aimeriez-vous aborder aujourd'hui ?"""
    }
    
    selected_starter = starters.get(scenario, starters["libre"])
    logger.info(f"Szenario-Starter '{scenario}' ausgewÃ¤hlt (Anfang): {selected_starter[:100]}...") # Loggen des Starters
    return selected_starter

def post_process_response(text, max_chars=200):

    """
    Verbesserte Nachbearbeitung fÃ¼r TTS-Optimierung

    """
    text = " ".join(text.split())

    if len(text) > max_chars:
        sentences = text.split('. ')
        result = ""

        for sentence in sentences:
            if len(result + sentence + '. ') <= max_chars:
                result += sentence + '. '
            else:
                break

        if not result.strip():
            result = text[:max_chars-3] + "..."  
        text = result.strip()
    
    if text and not text.endswith(('.', '!', '?')):
        text += '.'
    
    return text

# Modifizierte query_llm_mistral, um das neue first_turn_instruction_example-Argument zu akzeptieren und weiterzugeben

def query_llm_mistral(prompt, history=None, max_tokens=150, temperature=0.7, scenario="libre", first_turn_instruction_example=None):

    """
    Fragt Mistral LLM ab mit LÃ¤ngenbegrenzung und Konversationshistorie.
    Kann optional eine Anweisung fÃ¼r die erste Antwort Ã¼bergeben, die den System-Prompt ergÃ¤nzt.
    """

    api_key = os.environ.get("MISTRAL_API_KEY")

    if not api_key:

        raise Exception("MISTRAL_API_KEY Umgebungsvariable fehlt")

    url = "https://api.mistral.ai/v1/chat/completions"
    headers = {

        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    # Erweiterten System-Prompt fÃ¼r FranzÃ¶sisch-Tutor erstellen mit Beispiel je szenario
    system_prompt = get_scenario_system_prompt(scenario, first_turn_instruction_example=first_turn_instruction_example)

    logger.info(f"Finaler System Prompt fÃ¼r LLM (Anfang): {system_prompt[:100]}...") # Loggen des finalen System Prompts
    messages = [{"role": "system", "content": system_prompt}]

    # Sicherstellen, dass die Historie nicht den System-Prompt enthÃ¤lt, da dieser separat hinzugefÃ¼gt wird
    if history:
        messages.extend(history)
    
    messages.append({"role": "user", "content": prompt})

    payload = {
        "model": "mistral-small-latest",
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_p": 0.9,
        "random_seed": 42, # FÃ¼r reproduzierbare Ergebnisse, optional
        "stop": [".", "!", "?", "\n\n"]
    }
    
    #neu logging zentral in app.py:
    # Loggen der gesamten Konversation fÃ¼r Debugging/Ãœberwachung
    #logger.info("ðŸ—£ï¸ Komplette LLM-Konversation gesendet: %s", json.dumps(messages, indent=2, ensure_ascii=False))

    # Debugging: Nur erste und letzte Nachricht loggen (vom Benutzer bereitgestellt)
    #debug_messages = {
    #    "system": messages[0]["content"][:100] + "...",
    #    "last_user": messages[-1]["content"],
    #   "history_length": len(messages) - 2,  # Ohne System und aktuellen Prompt
    #    "scenario": scenario
    #}
    #logger.info("ðŸ§  LLM Mistral Request Summary: %s", json.dumps(debug_messages, indent=2, ensure_ascii=False))

    try:
        response = requests.post(url, headers=headers, json=payload, timeout=30)
        response.raise_for_status()
        
        result = response.json()
        content = result["choices"][0]["message"]["content"].strip()
        
        # Nachbearbeitung fÃ¼r TTS (bestehende Funktion post_process_response wird genutzt)
        content = post_process_response(content, max_chars=200)
        
        logger.info(f"âœ… Mistral response: {len(content)} characters")
        return content
        
    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ Mistral API error: {str(e)}")
        raise Exception(f"Mistral API Fehler: {str(e)}")
    except KeyError as e:
        logger.error(f"âŒ Unexpected response format: {result}")
        raise Exception("Unerwartetes Antwortformat von Mistral API")

# NEU: Funktion zum Abrufen der ersten LLM-Antwort fÃ¼r ein Szenario

def get_initial_llm_response_for_scenario(scenario):

    """
    Ruft die erste LLM-Antwort fÃ¼r ein neues Szenario ab,
    indem der Starter-Text als Anweisung in den System-Prompt integriert wird.
    """

    initial_prompt_for_llm = "Bonjour ! CommenÃ§ons notre conversation." 
    starter_example = get_scenario_starter(scenario) 

    first_llm_response = query_llm_mistral(

        prompt=initial_prompt_for_llm,
        history=None, 
        scenario=scenario,
        first_turn_instruction_example=starter_example 
    )

    return first_llm_response

def query_llm_for_scenario(prompt, scenario="libre", history=None, max_tokens=160):
    """
    Szenario-spezifische LLM-Abfrage mit optimierten Parametern
    """
    scenario_configs = {
        "restaurant": {"max_tokens": 120, "temperature": 0.6},
        "loisirs": {"max_tokens": 160, "temperature": 0.8},
        "travail": {"max_tokens": 140, "temperature": 0.5},
        "voyage": {"max_tokens": 150, "temperature": 0.7},
        "libre": {"max_tokens": 150, "temperature": 0.7}
    }
    
    config = scenario_configs.get(scenario, scenario_configs["libre"])
    logger.info(f"LLM-Konfiguration fÃ¼r Szenario '{scenario}': {config}")



    return query_llm_mistral(

        prompt, 
        history=history, 
        max_tokens=config["max_tokens"], 
        temperature=config["temperature"],
        scenario=scenario 

    )


# Diese Funktion wird im neuen Setup nicht mehr direkt als Intro verwendet, 
# sondern dient jetzt nur noch als Quelle fÃ¼r den Starter-Beispieltext.

def get_intro_for_scenario(scenario):

    return get_scenario_starter(scenario)