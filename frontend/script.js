// Optimiertes script.js mit verbesserter Audioaufnahme
document.addEventListener('DOMContentLoaded', function() {
  const elements = {
    recordBtn: document.getElementById('record'),
    stopBtn: document.getElementById('stop'),
    sendBtn: document.getElementById('sendMessage'),
    startBtn: document.getElementById('startConversation'),
    newConvBtn: document.getElementById('newConversation'),
    showResponseBtn: document.getElementById('showResponseBtn'),

    userText: document.getElementById('userText'),
    responseText: document.getElementById('responseText'),
    audioPlayback: document.getElementById('audioPlayback'),
    userAudio: document.getElementById('userAudio'),
    //userAudioSection: document.getElementById('userAudioSection'),

    startSection: document.getElementById('startSection'),
    conversationSection: document.getElementById('conversationSection'),
    scenarioSelect: document.getElementById('scenario'),
    recordingStatus: document.getElementById('recordingStatus'),
    audioStatus: document.getElementById('audioStatus')
  };

  let mediaRecorder;
  let audioChunks = [];
  let recognition;
  let recordedAudioBlob = null;
  let currentAudioStream = null;
  let currentUserId = null;
  let currentResponse = null;
  let audioHasBeenPlayed = false;
  let isTextCurrentlyVisible = false;
  let isRealTimeMode = true;
  let recognitionActive = false;
  let recognitionTimeout;
  let finalTranscript = '';
  let isRecognitionRestarting = false;
  let userId = Date.now().toString();
  let currentScenario = 'libre';
  let autoSendAfterRecording = false; // Konfig automatisches Senden der UserAufnahme
  let isRecording = false; // Status-Tracker
  let isPaused = false; // Neuer Status f√ºr Pause
  // NEU: Konversationshistorie
  let conversationHistory = []; // Speichert Nachrichten als {role: 'user'/'assistant', content: 'text'}

  const placeholderText = "Tapez votre message ici ou utilisez l'enregistrement...";

  // === VERBESSERTE Spracherkennung ===
  const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
  if (SpeechRecognition) {
    recognition = new SpeechRecognition();
    recognition.lang = 'fr-FR';
    recognition.interimResults = true;
    recognition.continuous = true;
    recognition.maxAlternatives = 1;

    recognition.onresult = (event) => {
      console.log('Speech recognition result received');
      let interimTranscript = '';
      let newFinalTranscript = '';
      
      for (let i = 0; i < event.results.length; i++) {
        const result = event.results[i];
        const transcript = result[0].transcript;
        
        if (result.isFinal) {
          newFinalTranscript += transcript + ' ';
          console.log('Final transcript added:', transcript);
        } else {
          interimTranscript += transcript;
        }
      }

      if (newFinalTranscript.trim()) {
        finalTranscript += newFinalTranscript;
        console.log('Updated final transcript:', finalTranscript);
      }

      const displayText = (finalTranscript + interimTranscript).trim();
      if (elements.userText && displayText) {
        elements.userText.textContent = displayText;
        elements.userText.classList.remove('placeholder');
        elements.userText.dataset.isPlaceholder = 'false';
      }

      const statusText = interimTranscript ? 
        `üé§ √âcoute... "${interimTranscript}"` : 
        (newFinalTranscript ? `üé§ Transcrit: "${newFinalTranscript.trim()}"` : 'üé§ En √©coute...');
      showStatus(elements.recordingStatus, statusText, 'success');
    };

    recognition.onerror = (event) => {
      console.error('Speech recognition error:', event.error);
      
      let errorMessage = '‚ö†Ô∏è Erreur de reconnaissance vocale';
      let shouldRestart = false;
      
      switch (event.error) {
        case 'not-allowed':
          errorMessage = 'üö´ Acc√®s au microphone refus√©';
          recognitionActive = false;
          break;
        case 'no-speech':
          console.log('No speech detected, will restart...');
          shouldRestart = true;
          errorMessage = null;
          break;
        case 'network':
          errorMessage = 'üåê Erreur r√©seau';
          shouldRestart = true;
          break;
        case 'aborted':
          return;
        default:
          shouldRestart = true;
          break;
      }
      
      recognitionActive = false;
      
      if (errorMessage) {
        showStatus(elements.recordingStatus, errorMessage, 'error');
      }
      
      if (shouldRestart && !isRecognitionRestarting && isRecording && !isPaused) {
            setTimeout(() => {
              if (!isRecognitionRestarting && isRecording && !isPaused) {
                startRecognition();
              }
            }, 1000);
          }
        };

        recognition.onend = () => {
          console.log('Speech recognition ended');
          recognitionActive = false;
          if (!isRecognitionRestarting && isRecording && !isPaused) {
            setTimeout(() => {
              if (!isRecognitionRestarting && !recognitionActive && isRecording && !isPaused) {
                startRecognition();
              }
            }, 500);
          }
        };

    recognition.onstart = () => {
      console.log('Speech recognition started');
      recognitionActive = true;
      isRecognitionRestarting = false;
    };

    function startRecognition() {
      if (isRecognitionRestarting || isPaused) {
        console.log('Recognition restart already in progress or paused');
        return;
      }

      if (recognitionActive) {
        console.log('Recognition already active, stopping first');
        isRecognitionRestarting = true;
        try {
          recognition.stop();
        } catch (e) {
          console.warn('Could not stop recognition:', e);
        }
        
        setTimeout(() => {
          if (isRealTimeMode && isRecording && !isPaused) {
            startRecognition();
          }
        }, 1000);
        return;
      }

      try {
        console.log('Starting speech recognition, current final transcript:', finalTranscript);
        isRecognitionRestarting = false;
        recognition.start();
        
        setTimeout(() => {
          console.log('Recognition active after start:', recognitionActive);
        }, 200); // HIER GE√ÑNDERT
        
      } catch (e) {
        console.error('Could not start recognition:', e);
        recognitionActive = false;
        isRecognitionRestarting = false;
        showStatus(elements.recordingStatus, '‚ö†Ô∏è Impossible de d√©marrer la reconnaissance vocale', 'error');
        
        if (isRealTimeMode && isRecording && !isPaused) {
          setTimeout(() => {
            if (isRealTimeMode && !recognitionActive && isRecording && !isPaused) {
              startRecognition();
            }
          }, 3000);
        }
      }
    }


  } else {
    console.warn('SpeechRecognition API nicht verf√ºgbar.');
    showStatus(elements.recordingStatus, '‚ö†Ô∏è Reconnaissance vocale non support√©e dans ce navigateur.', 'warning');
  }

    function stopRecognition() {
      if (recognitionActive) {
        try {
          recognition.stop();
        } catch (e) {
          console.warn('Could not stop recognition:', e);
        }
      }
      recognitionActive = false;
    }

    function resetRecordButton() {
      if (!elements.recordBtn) return;
      
      elements.recordBtn.innerHTML = 'üéôÔ∏è Enregistrer';
      elements.recordBtn.classList.remove('recording', 'paused');
      elements.recordBtn.disabled = false;
      
      if (elements.stopBtn) {
        elements.stopBtn.classList.add('hidden');
      }
    }

  // === VERBESSERTE Mikrofonzugriff-Diagnose ===
  async function checkMicrophonePermissions() {
    try {
      console.log('Checking microphone permissions...');
      
      if ('permissions' in navigator) {
        try {
          const permission = await navigator.permissions.query({name: 'microphone'});
          console.log('Microphone permission state:', permission.state);
          
          if (permission.state === 'denied') {
            showStatus(elements.recordingStatus, 'üö´ Acc√®s microphone refus√©. Activez-le dans les param√®tres du navigateur.', 'error');
            return false;
          }
        } catch (permError) {
          console.warn('Permission query failed:', permError);
        }
      }

      if (location.protocol !== 'https:' && !location.hostname.includes('localhost') && location.hostname !== '127.0.0.1') {
        showStatus(elements.recordingStatus, 'üîí HTTPS requis pour l\'acc√®s microphone.', 'error');
        return false;
      }

      try {
        const testStream = await navigator.mediaDevices.getUserMedia({ 
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true
          }
        });
        testStream.getTracks().forEach(track => track.stop());
        console.log('Microphone access test successful');
        return true;
      } catch (mediaError) {
        console.error('Microphone access test failed:', mediaError);
        let errorMsg = 'üéôÔ∏è Impossible d\'acc√©der au microphone';
        
        if (mediaError.name === 'NotAllowedError') {
          errorMsg += ': Permission refus√©e';
        } else if (mediaError.name === 'NotFoundError') {
          errorMsg += ': Aucun microphone trouv√©';
        } else {
          errorMsg += ': ' + mediaError.message;
        }
        
        showStatus(elements.recordingStatus, errorMsg, 'error');
        return false;
      }

    } catch (error) {
      console.error('Permission check failed:', error);
      return false;
    }
  }

  // Utility Functions
  function showStatus(element, message, type = 'loading') {
    if (!element) return;
    element.className = `status-message status-${type}`;
    element.innerHTML = message;
    element.classList.remove('hidden');
    console.log(`Status [${type}]:`, message);
  }

  function hideStatus(element) {
    if (!element) return;
    element.classList.add('hidden');
  }

  function showProgressStatus(step, message) {
    const progressBarHTML = `
      <div style="margin-bottom: 15px;">
        <div style="background: #e2e8f0; border-radius: 10px; height: 20px; overflow: hidden;">
          <div style="background: linear-gradient(90deg, #667eea, #764ba2); height: 100%; width: ${step * 25}%; transition: width 0.5s ease;"></div>
        </div>
        <div style="text-align: center; margin-top: 8px; font-weight: 500;">${message}</div>
      </div>
    `;
    
    if (elements.responseText) {
      elements.responseText.innerHTML = progressBarHTML;
      isTextCurrentlyVisible = false;
    }
  }

  function showResponseText() {
    if (currentResponse && elements.responseText) {
      elements.responseText.innerHTML = currentResponse;
      isTextCurrentlyVisible = true;
      updateShowResponseButton();
    }
  }

  function hideResponseText() {
    showProgressStatus(4, '‚úÖ Texte masqu√©. Cliquez pour afficher.');
    isTextCurrentlyVisible = false;
    updateShowResponseButton();
  }

  function updateShowResponseButton() {
    if (!elements.showResponseBtn) return;
    
    if (audioHasBeenPlayed && currentResponse) {
      elements.showResponseBtn.classList.remove('hidden');
      if (isTextCurrentlyVisible) {
        elements.showResponseBtn.innerHTML = 'üôà Masquer la r√©ponse';
      } else {
        elements.showResponseBtn.innerHTML = 'üëÅÔ∏è Afficher la r√©ponse';
      }
    } else {
      elements.showResponseBtn.classList.add('hidden');
    }
  }

  function resetUI() {
    console.log('Resetting UI...');
    
    // Stop recording completely
    isRecording = false;
    isPaused = false;
    
    if (recognition) {
      isRecognitionRestarting = true;
      try {
        recognition.stop();
      } catch (e) {
        console.warn('Could not stop recognition:', e);
      }
    }
    
    recognitionActive = false;
    isRecognitionRestarting = false;
    
    if (recognitionTimeout) {
      clearTimeout(recognitionTimeout);
    }

    // Stop MediaRecorder properly
    if (mediaRecorder && mediaRecorder.state !== 'inactive') {
      try {
        mediaRecorder.stop();
      } catch (e) {
        console.warn('Could not stop MediaRecorder:', e);
      }
    }
    
    cleanupAudioStream();

    elements.startSection?.classList.remove('hidden');
    elements.conversationSection?.classList.add('hidden');
    
    if (elements.userText) {
      elements.userText.textContent = placeholderText;
      elements.userText.classList.add('placeholder');
      elements.userText.dataset.isPlaceholder = 'true';
    }
    if (elements.responseText) {
      elements.responseText.textContent = '...';
    }
    
    if (elements.audioPlayback) {
      elements.audioPlayback.src = '';
      elements.audioPlayback.classList.add('hidden');
    }
    
    if (elements.userAudio) {
      elements.userAudio.src = '';
      elements.userAudio.classList.add('hidden');
    }
    
    //elements.userAudioSection?.classList.add('hidden');
    elements.showResponseBtn?.classList.add('hidden');
    
    updateRecordButton();
    
    currentUserId = null;
    recordedAudioBlob = null;
    currentResponse = null;
    audioHasBeenPlayed = false;
    isTextCurrentlyVisible = false;
    finalTranscript = '';
    audioChunks = []; // Reset audio chunks
    conversationHistory = []; // NEU: Konversationshistorie zur√ºcksetzen

    hideStatus(elements.recordingStatus);
    hideStatus(elements.audioStatus);
    hideStatus(elements.recordingStatus);
  }

  // Pause/Resume Funktionalit√§t f√ºr Aufnahme
  function pauseRealTimeSpeech() {
  console.log('Pausing real-time speech...');
  
  isPaused = true;
  isRecognitionRestarting = true;
  
  // Stop speech recognition
  if (recognition && recognitionActive) {
    try {
      recognition.stop();
    } catch (e) {
      console.warn('Could not stop recognition:', e);
    }
  }
  recognitionActive = false;
  // Pause MediaRecorder (keep it running but stop collecting meaningful data)
  if (mediaRecorder && mediaRecorder.state === "recording") {
    // MediaRecorder kann nicht pausiert werden, aber wir k√∂nnen die Erkennung stoppen
    console.log('Recording paused (speech recognition stopped)');
  }
  updateRecordButton();
  showStatus(elements.recordingStatus, '‚è∏Ô∏è Enregistrement en pause', 'loading');
  }

  function resumeRealTimeSpeech() {
      console.log('Resuming real-time speech...');
      
      isPaused = false;
      isRecognitionRestarting = false;
      
      // Resume speech recognition
      if (isRecording && recognition) {
        startRecognition();
      }
      
      updateRecordButton();
      showStatus(elements.recordingStatus, 'üé§ Enregistrement repris', 'success');
  }
  function updateRecordButton() {

    if (!elements.recordBtn) return;

    if (isRecording && !isPaused) {
      elements.recordBtn.innerHTML = '‚è∏Ô∏è Pause';
      elements.recordBtn.classList.add('recording');
      elements.recordBtn.classList.remove('paused');

    } else if (isRecording && isPaused) {
      elements.recordBtn.innerHTML = '‚ñ∂Ô∏è Reprendre';
      elements.recordBtn.classList.remove('recording');
      elements.recordBtn.classList.add('paused');

    } else {
      elements.recordBtn.innerHTML = 'üéôÔ∏è Enregistrer';
      elements.recordBtn.classList.remove('recording', 'paused');
    }

    elements.recordBtn.disabled = false;

    if (elements.stopBtn) {
      if (isRecording) {
        elements.stopBtn.classList.remove('hidden');
      } else {
        elements.stopBtn.classList.add('hidden');
      }
    }
  }

  // === VERBESSERTE Audioaufnahme-Funktion mit Pause/Resume===
    async function startRealTimeSpeech() {
      console.log('Starting real-time speech with recording...');
      
      try {
        const permissionsOk = await checkMicrophonePermissions();
        if (!permissionsOk || !recognition) {
          showStatus(elements.recordingStatus, '‚ö†Ô∏è Microphone ou reconnaissance vocale non disponibles', 'error');
          return;
        }

        // Set recording state
        isRecording = true;
        isPaused = false;
        
        // Reset transcript and audio
        finalTranscript = '';
        recordedAudioBlob = null;
        audioChunks = [];
        
        // Clear user text
        if (elements.userText) {
          elements.userText.textContent = '';
          elements.userText.classList.remove('placeholder');
          elements.userText.dataset.isPlaceholder = 'false';
        }
        
        // Get audio stream with optimized constraints
        const constraints = { 
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
            sampleRate: 44100,
            channelCount: 1
          }
        };
        
        currentAudioStream = await navigator.mediaDevices.getUserMedia(constraints);
        console.log('Audio stream obtained successfully');
        
        // Test audio stream
        const audioTracks = currentAudioStream.getAudioTracks();
        if (audioTracks.length === 0) {
          throw new Error('No audio tracks available');
        }
        
        console.log('Audio track settings:', audioTracks[0].getSettings());
        
        // Setup MediaRecorder with better options
        let options = { audioBitsPerSecond: 128000 };
        
        if (MediaRecorder.isTypeSupported('audio/webm;codecs=opus')) {
          options.mimeType = 'audio/webm;codecs=opus';
        } else if (MediaRecorder.isTypeSupported('audio/webm')) {
          options.mimeType = 'audio/webm';
        } else if (MediaRecorder.isTypeSupported('audio/mp4')) {
          options.mimeType = 'audio/mp4';
        } else {
          console.warn('No supported audio format found, using default');
          options = {};
        }
        
        console.log('Creating MediaRecorder with options:', options);
        mediaRecorder = new MediaRecorder(currentAudioStream, options);

        // Setup event handlers BEFORE starting
        mediaRecorder.ondataavailable = event => {
          console.log('MediaRecorder data available:', event.data.size, 'bytes');
          if (event.data.size > 0) {
            audioChunks.push(event.data);
            console.log('Total audio chunks:', audioChunks.length);
          }
        };

        mediaRecorder.onstop = async () => {
          console.log('MediaRecorder stopped, processing audio...');
          console.log('Total chunks collected:', audioChunks.length);
          
          if (audioChunks.length > 0) {
            const totalSize = audioChunks.reduce((sum, chunk) => sum + chunk.size, 0);
            console.log('Total audio size:', totalSize, 'bytes');
            
            if (totalSize > 0) {
              const mimeType = mediaRecorder.mimeType || 'audio/webm';
              recordedAudioBlob = new Blob(audioChunks, {type: mimeType});
              console.log('Created audio blob:', recordedAudioBlob.size, 'bytes, type:', recordedAudioBlob.type);
              
              // Pr√ºfe auf tats√§chlichen Audioinhalt
              showStatus(elements.recordingStatus, 'üîç Analyse de l\'audio...', 'loading');
              const hasContent = await hasAudioContent(recordedAudioBlob);
              
              if (hasContent) {
                showStatus(elements.recordingStatus, 'üíæ Sauvegarde audio...', 'loading');
                const uploadResult = await uploadRecordedAudio(recordedAudioBlob, mimeType);
                
                if (uploadResult && uploadResult.audio_path) {
                  showStatus(elements.recordingStatus, '‚úÖ Audio enregistr√©', 'success');
                  
                  if (elements.userAudio) {
                    elements.userAudio.src = uploadResult.audio_path;
                    elements.userAudio.load();
                    elements.userAudio.classList.remove('hidden');
                    console.log('User audio player configured:', elements.userAudio.src);
                  } else {

                    console.error('userAudio Element nicht gefunden!');

                  }
                } else {
                  showStatus(elements.recordingStatus, '‚ö†Ô∏è Erreur lors de l\'enregistrement de l\'audio', 'error');
                }
              } else {
                console.log('No significant audio content detected, skipping upload');
                showStatus(elements.recordingStatus, '‚ö†Ô∏è Aucun contenu audio d√©tect√©', 'warning');
                recordedAudioBlob = null;
              }
            } else {
              console.error('Audio chunks have zero total size!');
              showStatus(elements.recordingStatus, '‚ö†Ô∏è Aucun audio enregistr√©', 'error');
            }
          } else {
            console.error('No audio chunks recorded!');
            showStatus(elements.recordingStatus, '‚ö†Ô∏è Aucun audio enregistr√©', 'error');
          }
        };

        mediaRecorder.onerror = (event) => {
          console.error('MediaRecorder error:', event.error);
          showStatus(elements.recordingStatus, '‚ö†Ô∏è Erreur d\'enregistrement: ' + event.error, 'error');
        };

        mediaRecorder.onstart = () => {
          console.log('MediaRecorder started successfully');
          showStatus(elements.recordingStatus, 'üé§ Enregistrement actif', 'success');
        };
        
        // Start recording with smaller timeslices for better data collection
        console.log('Starting MediaRecorder...');
        mediaRecorder.start(250);
        
        // Start speech recognition
        isRecognitionRestarting = false;
        startRecognition();
        
        // Update UI
        updateRecordButton();
        
        showStatus(elements.recordingStatus, 'üé§ Enregistrement + d√©tection actifs', 'success');
        
      } catch (err) {
        console.error('Real-time speech error:', err);
        showStatus(elements.recordingStatus, '‚ö†Ô∏è Erreur: ' + err.message, 'error');
        isRecording = false;
        resetRecordButton();
        cleanupAudioStream();
      }
    }

function hasAudioContent(audioBlob) {
  return new Promise((resolve) => {
    const audioContext = new (window.AudioContext || window.webkitAudioContext)();
    const fileReader = new FileReader();
    
    fileReader.onload = function(e) {
      audioContext.decodeAudioData(e.target.result)
        .then(buffer => {
          // Pr√ºfe auf tats√§chlichen Audioinhalt
          let hasSound = false;
          const threshold = 0.01; // Mindestlautst√§rke
          
          for (let channel = 0; channel < buffer.numberOfChannels; channel++) {
            const channelData = buffer.getChannelData(channel);
            for (let i = 0; i < channelData.length; i++) {
              if (Math.abs(channelData[i]) > threshold) {
                hasSound = true;
                break;
              }
            }
            if (hasSound) break;
          }
          
          resolve(hasSound);
        })
        .catch(() => resolve(false));
    };
    
    fileReader.readAsArrayBuffer(audioBlob);
  });
}
    
  function stopRealTimeSpeech() {
    console.log('Stopping real-time speech...');
    
    // Set recording state
    isRecording = false;
    isRecognitionRestarting = true;
    
    // Stop speech recognition
    if (recognition && recognitionActive) {
      try {
        recognition.stop();
      } catch (e) {
        console.warn('Could not stop recognition:', e);
      }
    }
    recognitionActive = false;
    
    // Stop MediaRecorder
    if (mediaRecorder && mediaRecorder.state === "recording") {
      console.log('Stopping MediaRecorder...');
      try {
        mediaRecorder.stop();
        console.log('MediaRecorder stop called, state:', mediaRecorder.state);
      } catch (e) {
        console.error('Error stopping MediaRecorder:', e);
      }
    }
    
    // Clean up audio stream
    cleanupAudioStream();
    resetRecordButton();
    
    // Update user text with final transcript
    if (elements.userText) {
      const finalContent = finalTranscript.trim();
      elements.userText.textContent = finalContent;
      console.log('Final User Text set:', finalContent);

      if (finalContent) {
        elements.userText.classList.remove('placeholder');
        elements.userText.dataset.isPlaceholder = 'false';
      } else {
        elements.userText.textContent = placeholderText;
        elements.userText.classList.add('placeholder');
        elements.userText.dataset.isPlaceholder = 'true';
      }
    }


    // Send transcribed text to backend - NUR wenn konfiguriert
    if (finalTranscript.trim()) {
      if (autoSendAfterRecording) {
        console.log('Auto-Send aktiviert - gesendeter Text:', finalTranscript.trim());
        sendMessageToBackend(finalTranscript.trim());
      } else {
        showStatus(elements.recordingStatus, '‚úÖ Transcription pr√™te, Envoyer', 'success');
      }
    } else {
      showStatus(elements.recordingStatus, '‚ö†Ô∏è Aucune parole d√©tect√©e', 'warning');
    }

  }

  function cleanupAudioStream() {
    if (currentAudioStream) {
      console.log('Cleaning up audio stream...');
      currentAudioStream.getTracks().forEach(track => {
        track.stop();
        console.log('Audio track stopped');
      });
      currentAudioStream = null;
    }
  }

  // === Backend Communication ===

  // === Hilfsfunktion
  async function extractErrorMessage(response) {
      try {
        const text = await response.text();
        const parsed = JSON.parse(text);
        return parsed.error || parsed.response || text;
      } catch (e) {
        return "Erreur inconnue du serveur.";
      }
    }

async function sendMessageToBackend(message) {
    console.log('üì§ Sending message:', message);
    
    if (!message.trim()) {
        showStatus(elements.recordingStatus, 'Veuillez entrer un message.', 'warning');
        return;
    }
    
    showProgressStatus(1, 'üöÄ Message en cours d\'envoi...');
    elements.sendBtn.disabled = true;
    elements.recordBtn.disabled = true;

    try {
        // Vereinfachte Anfrage - Historie wird im Backend verwaltet
        const response = await fetch('/api/respond', {
            method: 'POST',
            headers: {'Content-Type': 'application/json'},
            body: JSON.stringify({
                message: message,
                userId: userId, 
                scenario: currentScenario
                // Entfernt: history Parameter
            }),
        });

        if (!response.ok) {
            const errorText = await extractErrorMessage(response);
            throw new Error(`HTTP error! status: ${response.status} - ${errorText}`);
        }

        const data = await response.json();
        console.log('‚úÖ Backend response received');
        currentResponse = data.response;
        
        // Lokale Historie nur zur Anzeige
        conversationHistory.push(
            { role: 'user', content: message },
            { role: 'assistant', content: data.response }
        );
        
        showResponseText();

        if (data.audio_url) {
            elements.audioPlayback.src = data.audio_url;
            elements.audioPlayback.classList.remove('hidden');
            audioHasBeenPlayed = false;
            updateShowResponseButton();
            showProgressStatus(4, 'üîä Texte et audio pr√™ts - 100% termin√©!');  
        } else {
            console.warn('‚ö†Ô∏è No audio URL received, starting retry...');
            showProgressStatus(3, '‚ö†Ô∏è Audio manquant - Nouvelle tentative...');
            
            setTimeout(async () => {
                try {
                    const retryResponse = await fetch('/api/respond', {
                        method: 'POST',
                        headers: {'Content-Type': 'application/json'},
                        body: JSON.stringify({
                            message: message,
                            userId: userId, 
                            scenario: currentScenario,
                            retry_audio: true
                        }),
                    });

                    if (!retryResponse.ok) {
                        throw new Error('Retry failed');
                    }
                    
                    const retryData = await retryResponse.json();
                    
                    if (retryData.audio_url) {
                        console.log('üîÑ Audio retry successful');
                        elements.audioPlayback.src = retryData.audio_url;
                        elements.audioPlayback.classList.remove('hidden');
                        showProgressStatus(4, 'üîä Texte et audio pr√™ts - 100% termin√©!');
                    } else {
                        throw new Error('No audio in retry');
                    }
                } catch (retryError) {
                    console.error('‚ùå Audio retry failed:', retryError);
                    showAudioRetryOptions();
                }
            }, 2000);
        }

        showStatus(elements.recordingStatus, '‚úÖ R√©ponse re√ßue', 'success');

    } catch (error) {
        console.error('‚ùå Error sending message:', error);
        showStatus(elements.recordingStatus, `‚ùå Erreur: ${error.message}`, 'error');
        elements.responseText.textContent = 'Erreur de communication du serveur';
        isTextCurrentlyVisible = true;
        updateShowResponseButton();
    } finally {
        elements.sendBtn.disabled = false;
        elements.recordBtn.disabled = false;
    }
}

  // === Audio Upload Function ===
  async function uploadRecordedAudio(audioBlob, mimeType) {
    if (!audioBlob || audioBlob.size === 0) {
      console.warn('No audio blob to upload or blob is empty.');
      return null;
    }

    const formData = new FormData();
    const fileExtension = mimeType.split('/')[1].split(';')[0];
    const fileName = `recording.${fileExtension}`;
    
    formData.append('audio', audioBlob, fileName);
    formData.append('user_id', userId);
    
    console.log(`Uploading audio blob: ${audioBlob.size} bytes, type: ${mimeType}, filename: ${fileName}`);

    try {
      const response = await fetch('/api/transcribe', {
        method: 'POST',
        body: formData,
      });

      if (!response.ok) {
        const errorText = await extractErrorMessage(response);
        throw new Error(`HTTP error! status: ${response.status} - ${errorText}`);
      }

      const data = await response.json();
      console.log('Audio uploaded successfully:', data);
      return data;

    } catch (error) {
      console.error('Error uploading audio:', error);
      return null;
    }
  }



  // === f√ºr Audio von TTS ===

    function showAudioRetryOptions() {
        if (elements.responseText) {
            elements.responseText.innerHTML = `
                <div style="text-align: center; margin-top: 15px;">
                    <div style="margin-bottom: 15px; color: #e74c3c;">‚ö†Ô∏è Audio manquant </div>
                    <button onclick="retryAudio()" style="margin-right: 10px; padding: 8px 16px; background: #3498db; color: white; border: none; border-radius: 5px; cursor: pointer;">
                        üîÑ R√©essayer
                    </button>
                    <button onclick="continueWithoutAudio()" style="padding: 8px 16px; background: #95a5a6; color: white; border: none; border-radius: 5px; cursor: pointer;">
                        ‚û°Ô∏è Continuer sans audio
                    </button>
                </div>
            `;
        }
    }

    function retryAudio() {
        console.log('Manueller Audio-Retry gestartet');
        showProgressStatus(3, 'üîÑ Nouvel essai...');
        // Nutzt die bestehende sendMessageToBackend Funktion
        sendMessageToBackend(elements.userText.textContent || finalTranscript);
    }

    function continueWithoutAudio() {
        console.log('Benutzer w√§hlt: ohne Audio fortfahren');
        showResponseText(); // Nutzt bestehende Funktion
        showProgressStatus(4, '‚úÖ Texte pr√™t - 100% termin√©!');
    } 

  // === Event Listeners ===
// === OPTIMIERTE KONVERSATIONS-STARTER ===
elements.startBtn?.addEventListener('click', async () => {
    console.log('üöÄ Starting conversation...');

    const scenario = elements.scenarioSelect?.value;
    if (!scenario) {
        showStatus(elements.recordingStatus, "‚ö†Ô∏è Veuillez choisir un th√®me.", 'error');
        return;
    }

    elements.startSection?.classList.add('hidden');
    elements.conversationSection?.classList.remove('hidden');
    currentUserId = userId;
    currentScenario = scenario;
    
    const currentScenarioDisplay = document.getElementById('currentScenarioDisplay');
    if (currentScenarioDisplay) {
        const scenarioNames = {
            "libre": "Conversation libre",
            "restaurant": "Au restaurant",
            "loisirs": "Loisirs et hobbies", 
            "travail": "Monde du travail",
            "voyage": "Voyage en France"
        };
        currentScenarioDisplay.innerText = scenarioNames[scenario] || scenario;
    }

    // Vereinfachte Conversation-Starter
    if (scenario !== "libre") {
        showProgressStatus(1, 'ü§î Pr√©paration de la conversation...');
        
        try {
            // Direkter Aufruf f√ºr Intro - keine LLM-Abfrage n√∂tig
            const response = await fetch('/api/start_conversation', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ 
                    scenario: scenario,
                    userId: userId
                })
            });

            if (!response.ok) {
                throw new Error(`HTTP error! status: ${response.status}`);
            }

            const data = await response.json();
            console.log('üéØ Conversation started successfully');
            
            currentResponse = data.response;
            conversationHistory = [{ role: 'assistant', content: data.response }];
            
            showProgressStatus(2, 'üìù Conversation pr√©par√©e...');
            
            if (data.audio_url) {
                elements.audioPlayback.src = data.audio_url;
                elements.audioPlayback.classList.remove('hidden');
                showProgressStatus(4, 'üîä Pr√™t! Cliquez pour √©couter.');
                
                elements.audioPlayback.addEventListener('ended', function handler() {
                    audioHasBeenPlayed = true;
                    showProgressStatus(4, '‚úÖ √Ä vous maintenant!');
                    updateShowResponseButton();
                    elements.audioPlayback.removeEventListener('ended', handler);
                }, { once: true });
            } else {
                audioHasBeenPlayed = true;
                showResponseText();
            }
            
        } catch (err) {
            console.error('‚ùå Error starting conversation:', err);
            showStatus(elements.recordingStatus, `‚ùå Erreur: ${err.message}`, 'error');
        }
    } else {
        // Freie Konversation
        currentResponse = "üéØ Conversation libre - parlez de ce qui vous int√©resse!";
        audioHasBeenPlayed = true;
        showResponseText();
    }
});

  elements.newConvBtn?.addEventListener('click', () => {
    resetUI();
  });

elements.recordBtn?.addEventListener('click', () => {
  if (isRecording && !isPaused) {
    pauseRealTimeSpeech();
  } else if (isRecording && isPaused) {
    resumeRealTimeSpeech();
  } else {
    startRealTimeSpeech();
  }
});

elements.audioPlayback?.addEventListener('play', () => {
  console.log('Audio playback started');
  audioHasBeenPlayed = true;
  updateShowResponseButton();
});
elements.audioPlayback?.addEventListener('ended', () => {
  console.log('Audio playback ended');
  audioHasBeenPlayed = true;
  updateShowResponseButton();
});

  elements.stopBtn?.addEventListener('click', () => {
    stopRealTimeSpeech();
  });

  elements.sendBtn?.addEventListener('click', () => {
    let messageToSend = '';
    console.log('=== SEND BUTTON CLICKED ===');
    console.log('finalTranscript:', finalTranscript);
    console.log('userText content:', elements.userText?.textContent);
    console.log('userText isPlaceholder:', elements.userText?.dataset.isPlaceholder);
    
    // Pr√ºfe ob userText ge√§ndert wurde (Priorit√§t √ºber finalTranscript)
    if (elements.userText?.textContent?.trim() && 
          elements.userText.dataset.isPlaceholder !== 'true' && 
          elements.userText.textContent !== placeholderText) {
        messageToSend = elements.userText.textContent.trim();
        console.log('Verwendung: Bearbeiteter userText');
    } else if (finalTranscript.trim()) {
        messageToSend = finalTranscript.trim();
        console.log('Verwendung: Original finalTranscript');
    }
      
    if (messageToSend) {
        console.log('Endg√ºltig gesendeter Text:', messageToSend);
        sendMessageToBackend(messageToSend);
    } else {
        console.log('Kein g√ºltiger Text zum Senden gefunden');
        showStatus(elements.recordingStatus, 'Veuillez d\'abord enregistrer ou taper un message.', 'warning');
    }
  });


  elements.showResponseBtn?.addEventListener('click', () => {
    if (currentResponse && audioHasBeenPlayed) {
      if (isTextCurrentlyVisible) {
        hideResponseText();
      } else {
        showResponseText();
      }
    } 
    else if (currentResponse && !audioHasBeenPlayed) {
        elements.showResponseBtn?.classList.add('hidden');
    }
    else if (!audioHasBeenPlayed) {
      showStatus(elements.recordingStatus, '‚ö†Ô∏è Veuillez d\'abord √©couter l\'audio', 'error');
      setTimeout(() => hideStatus(elements.recordingStatus), 3000);
    }
  });

    elements.scenarioSelect?.addEventListener('change', (event) => {
        currentScenario = event.target.value;
        console.log('Scenario changed to:', currentScenario);
    });

// === VERBESSERTES DEBUGGING ===
function debugConversationState() {
    console.log('=== CONVERSATION STATE DEBUG ===');
    console.log('üÜî User ID:', userId);
    console.log('üé≠ Current Scenario:', currentScenario);
    console.log('üìù Current Response:', currentResponse ? 'Set' : 'Not set');
    console.log('üéµ Audio played:', audioHasBeenPlayed);
    console.log('üëÅÔ∏è Text visible:', isTextCurrentlyVisible);
    console.log('üó£Ô∏è Recording:', isRecording);
    console.log('‚è∏Ô∏è Paused:', isPaused);
    console.log('üìú Local History Length:', conversationHistory.length);
    console.log('=================================');
}

// Debug-Funktion alle 30 Sekunden (nur in Development)
if (window.location.hostname === 'localhost' || window.location.hostname === '127.0.0.1') {
    setInterval(debugConversationState, 30000);
}
    

  // Keyboard shortcuts
document.addEventListener('keydown', (e) => {
  // Ctrl+Enter to send message
  if ((e.ctrlKey || e.metaKey) && e.key === 'Enter') {
    e.preventDefault();
    
    console.log('=== KEYBOARD SHORTCUT USED ===');
    let messageToSend = '';

    // Gleiche Logik wie beim Send Button
    if (elements.userText?.textContent?.trim() && 
        elements.userText.dataset.isPlaceholder !== 'true' && 
        elements.userText.textContent !== placeholderText) {
      messageToSend = elements.userText.textContent.trim();
      console.log('Verwendung: Bearbeiteter userText via Keyboard');
    } else if (finalTranscript.trim()) {
      messageToSend = finalTranscript.trim();
      console.log('Verwendung: Original finalTranscript via Keyboard');
    }

    if (messageToSend) {
      console.log('Endg√ºltig gesendeter Text via Keyboard:', messageToSend);
      sendMessageToBackend(messageToSend);
    } else {
      console.log('Kein g√ºltiger Text zum Senden gefunden via Keyboard');
      showStatus(elements.recordingStatus, 'Veuillez d\'abord enregistrer ou taper un message.', 'warning');
    }
  }
  
  // Space bar to pause/resume recording (when not in input field)
  if (e.code === 'Space' && e.target === document.body && elements.conversationSection && !elements.conversationSection.classList.contains('hidden')) {
    e.preventDefault();
    if (isRecording && !isPaused) {
      pauseRealTimeSpeech();
    } else if (isRecording && isPaused) {
      resumeRealTimeSpeech();
    } else {
      startRealTimeSpeech();
    }
  }
 
 }); 

// Initial UI setup
  resetUI();
  console.log('üöÄ FR-AI-Tutor Frontend initialized with Real-Time Speech Recognition');
});